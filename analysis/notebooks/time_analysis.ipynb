{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/Robert/Code/Python/F1-ATPase-simulation\n"
     ]
    }
   ],
   "source": [
    "%cd ../..\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from archive.python.LangevinGillespie import LangevinGillespie as LangevinGillespie_Legacy\n",
    "from src.utils.compute_transition_matrix import compute_transition_matrix\n",
    "from bin.f1sim import LangevinGillespie as LangevinGillespie_PybindWrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[m\u001b[m\u001b[1m\u001b[31mOS\u001b[m: \u001b[mDebian GNU/Linux 13 (trixie) x86_64\n",
      "\u001b[m\u001b[1m\u001b[31mKernel\u001b[m: \u001b[mLinux 6.6.87.2-microsoft-standard-WSL2\n",
      "\u001b[m\u001b[1m\u001b[31mCPU\u001b[m: \u001b[mAMD Ryzen 9 9950X (32) @ 4.30 GHz\n",
      "\u001b[m\u001b[1m\u001b[31mGPU 1\u001b[m: \u001b[mNVIDIA GeForce RTX 5080 (15.52 GiB) [Discrete]\n",
      "\u001b[m\u001b[1m\u001b[31mGPU 2\u001b[m: \u001b[mAMD Radeon(TM) Graphics (459.77 MiB) [Integrated]\n"
     ]
    }
   ],
   "source": [
    "!fastfetch --logo none --structure os:kernel:cpu:gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_simulation_params(LG):\n",
    "    LG.steps = 2000\n",
    "    LG.dt = 1e-6\n",
    "    LG.method = \"heun\"\n",
    "\n",
    "    # Mechanical / Thermal Setup\n",
    "    LG.kappa = 56\n",
    "    LG.kBT = 4.14\n",
    "    LG.gammaB = LG.computeGammaB(a=20, r=19, eta=1e-9)\n",
    "\n",
    "    # Multi State Setup\n",
    "    LG.theta_states = np.array([3, 36, 72, 116]) * math.pi / 180  # Deg â†’ Rad\n",
    "    LG.initial_state = 0  # Starting state\n",
    "\n",
    "    # Transition rate matrix\n",
    "    LG.transition_matrix = compute_transition_matrix(LG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 10000, total batches: 1\n"
     ]
    }
   ],
   "source": [
    "# Initialize simulation wrapper\n",
    "LG_PybindWrap = LangevinGillespie_PybindWrap()\n",
    "LG_Legacy = LangevinGillespie_Legacy()\n",
    "initialize_simulation_params(LG_PybindWrap)\n",
    "initialize_simulation_params(LG_Legacy)\n",
    "\n",
    "N_SIMS = 10000\n",
    "\n",
    "BYTES_PER_FLOAT = 4\n",
    "BYTES_PER_INT = 4\n",
    "TOTAL_FLOATS_PER_SIM = 3 * LG_PybindWrap.steps  # bead_positions, target_theta, etc.\n",
    "TOTAL_INTS_PER_SIM = LG_PybindWrap.steps  # states\n",
    "BYTES_PER_SIM = (\n",
    "    TOTAL_FLOATS_PER_SIM * BYTES_PER_FLOAT + TOTAL_INTS_PER_SIM * BYTES_PER_INT\n",
    ")\n",
    "\n",
    "# --- Dynamic GPU memory check ---\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    total_mem = torch.cuda.get_device_properties(0).total_memory\n",
    "    allocated = torch.cuda.memory_allocated(0)\n",
    "    reserved = torch.cuda.memory_reserved(0)\n",
    "    free_mem = total_mem - allocated - reserved\n",
    "\n",
    "    # Leave some room for other GPU usage\n",
    "    MAX_MEMORY_BYTES = int(free_mem * 0.5)\n",
    "else:\n",
    "    # fallback for CPU only\n",
    "    MAX_MEMORY_BYTES = 8 * 1024**3  # GB\n",
    "\n",
    "# Compute batch size\n",
    "BATCH_SIZE = max(1, min(N_SIMS, MAX_MEMORY_BYTES // BYTES_PER_SIM))\n",
    "total_batches = math.ceil(N_SIMS / BATCH_SIZE)\n",
    "\n",
    "print(f\"Batch size: {BATCH_SIZE}, total batches: {total_batches}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: If you use multi-threading, avoid using swap memory, instead employ batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running batch 1/1: 10000 simulations\n",
      "Total time: 0.70 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "for batch_start in range(0, N_SIMS, BATCH_SIZE):\n",
    "    n_batch = min(BATCH_SIZE, N_SIMS - batch_start)\n",
    "    batch_num = batch_start // BATCH_SIZE + 1\n",
    "\n",
    "    print(f\"\\rRunning batch {batch_num}/{total_batches}: {n_batch} simulations\", end=\"\", flush=True)\n",
    "\n",
    "    # Run CUDA kernel\n",
    "    beads, states, thetas = LG_PybindWrap.simulate_multithreaded_cuda(n_batch)\n",
    "\n",
    "print(f\"\\nTotal time: {time.time() - start_time:.2f} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running batch 1/1: 10000 simulations\n",
      "Total time: 2.26 s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for batch_start in range(0, N_SIMS, BATCH_SIZE):\n",
    "    n_batch = min(BATCH_SIZE, N_SIMS - batch_start)\n",
    "    batch_num = batch_start // BATCH_SIZE + 1\n",
    "\n",
    "    print(f\"\\rRunning batch {batch_num}/{total_batches}: {n_batch} simulations\", end=\"\", flush=True)\n",
    "\n",
    "    beads, states, thetas = LG_PybindWrap.simulate_multithreaded(n_batch, 32)\n",
    "\n",
    "print(f\"\\nTotal time: {time.time() - start_time:.2f} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running batch 1/1: 10000 simulations\n",
      "Total time: 1.76 s\n"
     ]
    }
   ],
   "source": [
    "bead_store = []\n",
    "states_store = []\n",
    "thetas_store = []\n",
    "start_time = time.time()\n",
    "for batch_start in range(0, N_SIMS, BATCH_SIZE):\n",
    "    n_batch = min(BATCH_SIZE, N_SIMS - batch_start)\n",
    "    batch_num = batch_start // BATCH_SIZE + 1\n",
    "\n",
    "    print(f\"\\rRunning batch {batch_num}/{total_batches}: {n_batch} simulations\", end=\"\", flush=True)\n",
    "\n",
    "    for i in range(0, n_batch):\n",
    "        beads, states, thetas = LG_PybindWrap.simulate()\n",
    "        bead_store.append(beads)\n",
    "        states_store.append(states)\n",
    "        thetas_store.append(thetas)\n",
    "\n",
    "print(f\"\\nTotal time: {time.time() - start_time:.2f} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running batch 1/1: 10000 simulations\n",
      "Total time: 100.07 s\n"
     ]
    }
   ],
   "source": [
    "bead_store = []\n",
    "states_store = []\n",
    "thetas_store = []\n",
    "start_time = time.time()\n",
    "for batch_start in range(0, N_SIMS, BATCH_SIZE):\n",
    "    n_batch = min(BATCH_SIZE, N_SIMS - batch_start)\n",
    "    batch_num = batch_start // BATCH_SIZE + 1\n",
    "\n",
    "    print(f\"\\rRunning batch {batch_num}/{total_batches}: {n_batch} simulations\", end=\"\", flush=True)\n",
    "\n",
    "    for i in range(0, n_batch):\n",
    "        beads, states, thetas = LG_Legacy.simulate()\n",
    "        bead_store.append(beads)\n",
    "        states_store.append(states)\n",
    "        thetas_store.append(thetas)\n",
    "\n",
    "print(f\"\\nTotal time: {time.time() - start_time:.2f} s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
